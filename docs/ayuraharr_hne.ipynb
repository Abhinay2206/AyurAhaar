{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuJwgnuXARfe",
    "outputId": "173a7b82-6e5b-4a2b-ffaf-26b23a053869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfb1Vz6kM5iK",
    "outputId": "38f9fa6e-d055-4d4c-b80d-8ec414258ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fADUuTJdHMQd",
    "outputId": "b6fca0f7-1971-4284-f049-0f9824ab2f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🌿 Ayurveda Meal Planning System\n",
      "============================================================\n",
      "\n",
      "📁 Checking data files...\n",
      "  ✓ foods CSV found\n",
      "  ✓ patients CSV found\n",
      "  ✓ doctor_plans CSV found\n",
      "\n",
      "📊 Loading data...\n",
      "  ✓ Loaded 20 foods\n",
      "  ✓ Loaded 5 patients\n",
      "  ✓ Loaded 10 meal plans\n",
      "\n",
      "🤖 Initializing AI engine...\n",
      "Using device: cuda\n",
      "  ✓ Model loaded: t5-small\n",
      "\n",
      "🔗 Building knowledge graph...\n",
      "Building knowledge graph with 20 foods and 5 patients...\n",
      "  ✓ Graph built with 44 nodes\n",
      "\n",
      "📚 Preparing training dataset...\n",
      "  ✓ Train set: 9 samples\n",
      "  ✓ Validation set: 1 samples\n",
      "\n",
      "🎯 Fine-tuning model...\n",
      "Starting fine-tuning for 2 epochs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model fine-tuned and saved to ./ayurveda_meal_planner\n",
      "\n",
      "🍽️ Generating meal plans...\n",
      "------------------------------------------------------------\n",
      "\n",
      "👤 Patient 1:\n",
      "   Age: 25, Gender: Female\n",
      "   BMI: 24.2, Prakriti: Vata\n",
      "\n",
      "   📅 Day 1 Meal Plan:\n",
      "   🌅 Breakfast: oatmeal, fruits, milk\n",
      "   ☀️  Lunch: rice, dal, vegetables, yogurt\n",
      "   🌙 Dinner: chapati, vegetables, soup\n",
      "   🍎 Snacks: nuts, fruits\n",
      "\n",
      "👤 Patient 2:\n",
      "   Age: 35, Gender: Male\n",
      "   BMI: 24.2, Prakriti: Pitta\n",
      "   Conditions: ['hypertension']\n",
      "\n",
      "   📅 Day 1 Meal Plan:\n",
      "   🌅 Breakfast: oatmeal, fruits, milk\n",
      "   ☀️  Lunch: rice, dal, vegetables, yogurt\n",
      "   🌙 Dinner: chapati, vegetables, soup\n",
      "   🍎 Snacks: nuts, fruits\n",
      "\n",
      "👤 Patient 3:\n",
      "   Age: 45, Gender: Female\n",
      "   BMI: 24.2, Prakriti: Kapha\n",
      "   Conditions: ['diabetes']\n",
      "\n",
      "   📅 Day 1 Meal Plan:\n",
      "   🌅 Breakfast: oatmeal, fruits, milk\n",
      "   ☀️  Lunch: rice, dal, vegetables, yogurt\n",
      "   🌙 Dinner: chapati, vegetables, soup\n",
      "   🍎 Snacks: nuts, fruits\n",
      "\n",
      "👤 Patient 4:\n",
      "   Age: 30, Gender: Male\n",
      "   BMI: 24.2, Prakriti: Vata-Pitta\n",
      "\n",
      "   📅 Day 1 Meal Plan:\n",
      "   🌅 Breakfast: oatmeal, fruits, milk\n",
      "   ☀️  Lunch: rice, dal, vegetables, yogurt\n",
      "   🌙 Dinner: chapati, vegetables, soup\n",
      "   🍎 Snacks: nuts, fruits\n",
      "\n",
      "👤 Patient 5:\n",
      "   Age: 40, Gender: Female\n",
      "   BMI: 24.2, Prakriti: Pitta-Kapha\n",
      "   Conditions: ['arthritis']\n",
      "\n",
      "   📅 Day 1 Meal Plan:\n",
      "   🌅 Breakfast: oatmeal, fruits, milk\n",
      "   ☀️  Lunch: rice, dal, vegetables, yogurt\n",
      "   🌙 Dinner: chapati, vegetables, soup\n",
      "   🍎 Snacks: nuts, fruits\n",
      "\n",
      "============================================================\n",
      "✅ Meal plan generation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphSAGE, GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face transformers for pretrained models\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    T5ForConditionalGeneration, T5Tokenizer,\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    BartForConditionalGeneration, BartTokenizer,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from huggingface_hub import login as hf_login\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# Data structures for our domain\n",
    "@dataclass\n",
    "class Food:\n",
    "    id: str\n",
    "    name: str\n",
    "    category: str\n",
    "    calories: float\n",
    "    protein: float\n",
    "    carbs: float\n",
    "    fats: float\n",
    "    fiber: float\n",
    "    vitamins: Dict[str, float]\n",
    "    minerals: Dict[str, float]\n",
    "    dosha_effects: Dict[str, str]  # vata, pitta, kapha effects\n",
    "    rasa: str  # taste\n",
    "    guna: List[str]  # qualities\n",
    "    virya: str  # potency\n",
    "    vipaka: str  # post-digestive effect\n",
    "    health_tags: List[str]\n",
    "    contraindications: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Patient:\n",
    "    id: str\n",
    "    age: int\n",
    "    gender: str\n",
    "    weight: float\n",
    "    height: float\n",
    "    bmi: float\n",
    "    lifestyle: str\n",
    "    prakriti: str  # constitutional type\n",
    "    health_conditions: List[str]\n",
    "    allergies: List[str]\n",
    "    preferred_cuisine: List[str]\n",
    "\n",
    "@dataclass\n",
    "class MealPlan:\n",
    "    patient_id: str\n",
    "    day: int\n",
    "    breakfast: List[str]\n",
    "    lunch: List[str]\n",
    "    dinner: List[str]\n",
    "    snacks: List[str]\n",
    "    restrictions: List[str]\n",
    "    doctor_notes: str\n",
    "\n",
    "class NodeType(Enum):\n",
    "    FOOD = \"food\"\n",
    "    PATIENT = \"patient\"\n",
    "    DOSHA = \"dosha\"\n",
    "    RASA = \"rasa\"\n",
    "    GUNA = \"guna\"\n",
    "    CONDITION = \"condition\"\n",
    "    CATEGORY = \"category\"\n",
    "\n",
    "# Knowledge Graph Foundation\n",
    "class AyurvedaKnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.node_to_idx = {}\n",
    "        self.idx_to_node = {}\n",
    "        self.node_types = {}\n",
    "        self.node_features = {}\n",
    "        self.food_names_by_category = {}  # Store food names by category\n",
    "\n",
    "    def add_food_node(self, food: Food):\n",
    "        \"\"\"Add a food item and its relationships to the graph\"\"\"\n",
    "        food_id = f\"food_{food.id}\"\n",
    "        self.graph.add_node(food_id)\n",
    "        self.node_types[food_id] = NodeType.FOOD\n",
    "\n",
    "        # Store food name for recommendation\n",
    "        if food.category not in self.food_names_by_category:\n",
    "            self.food_names_by_category[food.category] = []\n",
    "        self.food_names_by_category[food.category].append(food.name)\n",
    "\n",
    "        # Store food features\n",
    "        self.node_features[food_id] = {\n",
    "            'calories': food.calories,\n",
    "            'protein': food.protein,\n",
    "            'carbs': food.carbs,\n",
    "            'fats': food.fats,\n",
    "            'fiber': food.fiber,\n",
    "            'category_embedding': self._encode_category(food.category)\n",
    "        }\n",
    "\n",
    "        # Add relationships\n",
    "        for dosha, effect in food.dosha_effects.items():\n",
    "            dosha_node = f\"dosha_{dosha}\"\n",
    "            self._ensure_node_exists(dosha_node, NodeType.DOSHA)\n",
    "            self.graph.add_edge(food_id, dosha_node, relation=f\"affects_{effect}\")\n",
    "\n",
    "        if food.rasa:\n",
    "            rasa_node = f\"rasa_{food.rasa}\"\n",
    "            self._ensure_node_exists(rasa_node, NodeType.RASA)\n",
    "            self.graph.add_edge(food_id, rasa_node, relation=\"has_taste\")\n",
    "\n",
    "        for guna in food.guna:\n",
    "            guna_node = f\"guna_{guna}\"\n",
    "            self._ensure_node_exists(guna_node, NodeType.GUNA)\n",
    "            self.graph.add_edge(food_id, guna_node, relation=\"has_quality\")\n",
    "\n",
    "        category_node = f\"category_{food.category}\"\n",
    "        self._ensure_node_exists(category_node, NodeType.CATEGORY)\n",
    "        self.graph.add_edge(food_id, category_node, relation=\"belongs_to\")\n",
    "\n",
    "        for tag in food.health_tags:\n",
    "            condition_node = f\"condition_{tag}\"\n",
    "            self._ensure_node_exists(condition_node, NodeType.CONDITION)\n",
    "            self.graph.add_edge(food_id, condition_node, relation=\"beneficial_for\")\n",
    "\n",
    "    def add_patient_node(self, patient: Patient):\n",
    "        \"\"\"Add patient and their characteristics\"\"\"\n",
    "        patient_id = f\"patient_{patient.id}\"\n",
    "        self.graph.add_node(patient_id)\n",
    "        self.node_types[patient_id] = NodeType.PATIENT\n",
    "\n",
    "        self.node_features[patient_id] = {\n",
    "            'age': patient.age,\n",
    "            'bmi': patient.bmi,\n",
    "            'gender_embedding': self._encode_gender(patient.gender),\n",
    "            'lifestyle_embedding': self._encode_lifestyle(patient.lifestyle)\n",
    "        }\n",
    "\n",
    "        prakriti_node = f\"dosha_{patient.prakriti}\"\n",
    "        self._ensure_node_exists(prakriti_node, NodeType.DOSHA)\n",
    "        self.graph.add_edge(patient_id, prakriti_node, relation=\"has_prakriti\")\n",
    "\n",
    "        for condition in patient.health_conditions:\n",
    "            condition_node = f\"condition_{condition}\"\n",
    "            self._ensure_node_exists(condition_node, NodeType.CONDITION)\n",
    "            self.graph.add_edge(patient_id, condition_node, relation=\"has_condition\")\n",
    "\n",
    "    def _ensure_node_exists(self, node_id: str, node_type: NodeType):\n",
    "        if node_id not in self.graph:\n",
    "            self.graph.add_node(node_id)\n",
    "            self.node_types[node_id] = node_type\n",
    "            self.node_features[node_id] = {}\n",
    "\n",
    "    def _encode_category(self, category: str) -> List[float]:\n",
    "        categories = ['grains', 'vegetables', 'fruits', 'dairy', 'spices', 'legumes', 'nuts', 'oils']\n",
    "        encoding = [1.0 if category.lower() == cat else 0.0 for cat in categories]\n",
    "        if sum(encoding) == 0:  # Unknown category\n",
    "            encoding.append(1.0)\n",
    "        else:\n",
    "            encoding.append(0.0)\n",
    "        return encoding\n",
    "\n",
    "    def _encode_gender(self, gender: str) -> List[float]:\n",
    "        return [1.0, 0.0] if gender.lower() == 'male' else [0.0, 1.0]\n",
    "\n",
    "    def _encode_lifestyle(self, lifestyle: str) -> List[float]:\n",
    "        lifestyles = ['sedentary', 'moderate', 'active', 'very_active']\n",
    "        encoding = [1.0 if lifestyle.lower() == ls else 0.0 for ls in lifestyles]\n",
    "        return encoding\n",
    "\n",
    "    def to_pytorch_geometric(self) -> Data:\n",
    "        \"\"\"Convert NetworkX graph to PyTorch Geometric format\"\"\"\n",
    "        nodes = list(self.graph.nodes())\n",
    "        self.node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        self.idx_to_node = {idx: node for node, idx in self.node_to_idx.items()}\n",
    "\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_index = torch.tensor([[self.node_to_idx[u], self.node_to_idx[v]]\n",
    "                                  for u, v in edges], dtype=torch.long).t().contiguous()\n",
    "\n",
    "        node_features = []\n",
    "        for node in nodes:\n",
    "            features = []\n",
    "            node_type = self.node_types.get(node, NodeType.FOOD)\n",
    "\n",
    "            type_embedding = [0.0] * len(NodeType)\n",
    "            type_embedding[list(NodeType).index(node_type)] = 1.0\n",
    "            features.extend(type_embedding)\n",
    "\n",
    "            if node in self.node_features:\n",
    "                for key, value in self.node_features[node].items():\n",
    "                    if isinstance(value, list):\n",
    "                        features.extend(value)\n",
    "                    else:\n",
    "                        features.append(float(value))\n",
    "\n",
    "            while len(features) < 32:\n",
    "                features.append(0.0)\n",
    "\n",
    "            node_features.append(features[:32])\n",
    "\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Graph Neural Network for Food Embeddings\n",
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        h1 = F.relu(self.conv1(x, edge_index))\n",
    "        h1 = self.dropout(h1)\n",
    "        h2 = F.relu(self.conv2(h1, edge_index))\n",
    "        h2 = self.dropout(h2)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        return h3\n",
    "\n",
    "# Pretrained Transformer Models with Better Generation\n",
    "class T5MealPlanner(nn.Module):\n",
    "    \"\"\"Using T5 for text-to-text meal planning\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"t5-small\", graph_embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        # Add special tokens for Ayurveda concepts\n",
    "        special_tokens = [\n",
    "            \"<patient>\", \"</patient>\", \"<day>\", \"</day>\",\n",
    "            \"<breakfast>\", \"</breakfast>\", \"<lunch>\", \"</lunch>\",\n",
    "            \"<dinner>\", \"</dinner>\", \"<snacks>\", \"</snacks>\",\n",
    "            \"<vata>\", \"<pitta>\", \"<kapha>\",\n",
    "            \"<diabetes>\", \"<hypertension>\", \"<obesity>\", \"<digestion>\"\n",
    "        ]\n",
    "\n",
    "        self.tokenizer.add_tokens(special_tokens)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # Graph embeddings integration\n",
    "        self.graph_encoder = GraphNeuralNetwork(32, 128, graph_embedding_dim)\n",
    "        self.graph_projection = nn.Linear(graph_embedding_dim, self.model.config.d_model)\n",
    "\n",
    "    def format_patient_input(self, patient: Patient, day: int) -> str:\n",
    "        \"\"\"Convert patient data to structured text input\"\"\"\n",
    "        input_text = \"generate meal plan: \"\n",
    "        input_text += f\"patient age {patient.age} gender {patient.gender} \"\n",
    "        input_text += f\"bmi {patient.bmi:.1f} lifestyle {patient.lifestyle} \"\n",
    "        input_text += f\"prakriti {patient.prakriti} \"\n",
    "\n",
    "        if patient.health_conditions:\n",
    "            conditions = \" \".join(patient.health_conditions)\n",
    "            input_text += f\"conditions {conditions} \"\n",
    "\n",
    "        if patient.allergies:\n",
    "            input_text += f\"allergies {' '.join(patient.allergies)} \"\n",
    "\n",
    "        input_text += f\"day {day}\"\n",
    "        return input_text\n",
    "\n",
    "    def format_meal_plan_output(self, meal_plan: MealPlan) -> str:\n",
    "        \"\"\"Convert meal plan to structured text output\"\"\"\n",
    "        output_text = \"\"\n",
    "        if meal_plan.breakfast:\n",
    "            output_text += \"breakfast: \" + \", \".join(meal_plan.breakfast) + \" \"\n",
    "        if meal_plan.lunch:\n",
    "            output_text += \"lunch: \" + \", \".join(meal_plan.lunch) + \" \"\n",
    "        if meal_plan.dinner:\n",
    "            output_text += \"dinner: \" + \", \".join(meal_plan.dinner) + \" \"\n",
    "        if meal_plan.snacks:\n",
    "            output_text += \"snacks: \" + \", \".join(meal_plan.snacks)\n",
    "        return output_text.strip()\n",
    "\n",
    "# Dataset class for training\n",
    "class AyurvedaMealPlanDataset(Dataset):\n",
    "    def __init__(self, patients: List[Patient], meal_plans: List[MealPlan],\n",
    "                 tokenizer, max_length: int = 512, model_type: str = \"t5\"):\n",
    "        self.patients = {p.id: p for p in patients}\n",
    "        self.meal_plans = meal_plans\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meal_plans)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meal_plan = self.meal_plans[idx]\n",
    "        patient = self.patients.get(meal_plan.patient_id)\n",
    "\n",
    "        if not patient:\n",
    "            # Create a dummy patient if not found\n",
    "            patient = Patient(\n",
    "                id=meal_plan.patient_id,\n",
    "                age=30, gender=\"unknown\", weight=70, height=170, bmi=24,\n",
    "                lifestyle=\"moderate\", prakriti=\"vata\",\n",
    "                health_conditions=[], allergies=[], preferred_cuisine=[]\n",
    "            )\n",
    "\n",
    "        # Simplified format for better training\n",
    "        input_text = f\"generate meal plan: age {patient.age} {patient.gender} \"\n",
    "        input_text += f\"bmi {patient.bmi:.1f} {patient.prakriti} day {meal_plan.day}\"\n",
    "\n",
    "        # Simplified output format\n",
    "        target_text = \"\"\n",
    "        if meal_plan.breakfast:\n",
    "            target_text += f\"breakfast: {', '.join(meal_plan.breakfast)} \"\n",
    "        if meal_plan.lunch:\n",
    "            target_text += f\"lunch: {', '.join(meal_plan.lunch)} \"\n",
    "        if meal_plan.dinner:\n",
    "            target_text += f\"dinner: {', '.join(meal_plan.dinner)} \"\n",
    "        if meal_plan.snacks:\n",
    "            target_text += f\"snacks: {', '.join(meal_plan.snacks)}\"\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Replace padding token id with -100 for loss calculation\n",
    "        targets['input_ids'][targets['input_ids'] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Data loading utilities\n",
    "def _split_list(val: Optional[str]) -> List[str]:\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # try JSON array\n",
    "    try:\n",
    "        parsed = json.loads(s)\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: split by | or ,\n",
    "    sep = '|' if '|' in s else ','\n",
    "    return [p.strip() for p in s.split(sep) if p.strip()]\n",
    "\n",
    "def _parse_float(val, default: float = 0.0) -> float:\n",
    "    try:\n",
    "        if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "            return default\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _parse_dict(val) -> Dict[str, str]:\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return {}\n",
    "    if isinstance(val, dict):\n",
    "        return {str(k): str(v) for k, v in val.items()}\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return {}\n",
    "    # try json\n",
    "    try:\n",
    "        parsed = json.loads(s)\n",
    "        if isinstance(parsed, dict):\n",
    "            return {str(k): str(v) for k, v in parsed.items()}\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: key:value pairs\n",
    "    pairs = s.split('|') if '|' in s else s.split(',')\n",
    "    out = {}\n",
    "    for p in pairs:\n",
    "        if ':' in p:\n",
    "            k, v = p.split(':', 1)\n",
    "            out[k.strip()] = v.strip()\n",
    "    return out\n",
    "\n",
    "def load_foods_csv(path: str) -> List[Food]:\n",
    "    df = pd.read_csv(path)\n",
    "    foods = []\n",
    "    for idx, row in df.iterrows():\n",
    "        foods.append(\n",
    "            Food(\n",
    "                id=str(row.get('id', row.get('food_id', idx))),\n",
    "                name=str(row.get('name', row.get('food_name', 'Unknown'))),\n",
    "                category=str(row.get('category', 'unknown')),\n",
    "                calories=_parse_float(row.get('calories', 0)),\n",
    "                protein=_parse_float(row.get('protein', 0)),\n",
    "                carbs=_parse_float(row.get('carbs', row.get('carbohydrates', 0))),\n",
    "                fats=_parse_float(row.get('fats', row.get('fat', 0))),\n",
    "                fiber=_parse_float(row.get('fiber', 0)),\n",
    "                vitamins=_parse_dict(row.get('vitamins', {})),\n",
    "                minerals=_parse_dict(row.get('minerals', {})),\n",
    "                dosha_effects=_parse_dict(row.get('dosha_effects', {})),\n",
    "                rasa=str(row.get('rasa', 'sweet')),\n",
    "                guna=_split_list(row.get('guna', row.get('qualities', ''))),\n",
    "                virya=str(row.get('virya', 'neutral')),\n",
    "                vipaka=str(row.get('vipaka', 'sweet')),\n",
    "                health_tags=_split_list(row.get('health_tags', row.get('tags', ''))),\n",
    "                contraindications=_split_list(row.get('contraindications', '')),\n",
    "            )\n",
    "        )\n",
    "    return foods\n",
    "\n",
    "def load_patients_csv(path: str) -> List[Patient]:\n",
    "    df = pd.read_csv(path)\n",
    "    patients = []\n",
    "    for idx, row in df.iterrows():\n",
    "        height = _parse_float(row.get('height', 170))\n",
    "        weight = _parse_float(row.get('weight', 70))\n",
    "        bmi = _parse_float(row.get('bmi', 0))\n",
    "        if not bmi and height and weight:\n",
    "            try:\n",
    "                h_m = height / 100.0 if height > 3 else height\n",
    "                bmi = weight / (h_m * h_m) if h_m else 24.0\n",
    "            except Exception:\n",
    "                bmi = 24.0\n",
    "        patients.append(\n",
    "            Patient(\n",
    "                id=str(row.get('id', row.get('patient_id', idx))),\n",
    "                age=int(_parse_float(row.get('age', 30))),\n",
    "                gender=str(row.get('gender', 'unknown')),\n",
    "                weight=weight,\n",
    "                height=height,\n",
    "                bmi=bmi,\n",
    "                lifestyle=str(row.get('lifestyle', 'moderate')),\n",
    "                prakriti=str(row.get('prakriti', row.get('constitution', 'vata'))),\n",
    "                health_conditions=_split_list(row.get('health_conditions', row.get('conditions', ''))),\n",
    "                allergies=_split_list(row.get('allergies', '')),\n",
    "                preferred_cuisine=_split_list(row.get('preferred_cuisine', row.get('cuisine', ''))),\n",
    "            )\n",
    "        )\n",
    "    return patients\n",
    "\n",
    "def load_doctor_plans_csv(path: str) -> List[MealPlan]:\n",
    "    df = pd.read_csv(path)\n",
    "    plans = []\n",
    "    for idx, row in df.iterrows():\n",
    "        plans.append(\n",
    "            MealPlan(\n",
    "                patient_id=str(row.get('patient_id', row.get('id', idx))),\n",
    "                day=int(_parse_float(row.get('day', 1))),\n",
    "                breakfast=_split_list(row.get('breakfast', '')),\n",
    "                lunch=_split_list(row.get('lunch', '')),\n",
    "                dinner=_split_list(row.get('dinner', '')),\n",
    "                snacks=_split_list(row.get('snacks', '')),\n",
    "                restrictions=_split_list(row.get('restrictions', '')),\n",
    "                doctor_notes=str(row.get('doctor_notes', row.get('notes', ''))),\n",
    "            )\n",
    "        )\n",
    "    return plans\n",
    "\n",
    "def huggingface_login(token: Optional[str] = None):\n",
    "    \"\"\"Login to Hugging Face using a token\"\"\"\n",
    "    tok = token or os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HF_TOKEN')\n",
    "    if tok:\n",
    "        try:\n",
    "            hf_login(token=tok)\n",
    "            print('✓ Logged in to Hugging Face Hub')\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Hugging Face login failed: {e}\")\n",
    "    else:\n",
    "        print('ℹ No Hugging Face token provided; proceeding without login')\n",
    "\n",
    "# Define compute_metrics function for evaluation\n",
    "def compute_metrics(p: EvalPrediction) -> Dict[str, float]:\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    # Simple evaluation - could be enhanced with ROUGE, BLEU, etc.\n",
    "    # For now, just return loss\n",
    "    return {\"eval_loss\": p.metrics[\"eval_loss\"]}\n",
    "\n",
    "# Main Hybrid Neural Engine\n",
    "class HybridNeuralEngine:\n",
    "    def __init__(self, model_type: str = \"t5\", model_name: str = None):\n",
    "        self.model_type = model_type\n",
    "        self.knowledge_graph = AyurvedaKnowledgeGraph()\n",
    "\n",
    "        # Use smaller models for faster inference\n",
    "        if model_type == \"t5\":\n",
    "            model_name = model_name or \"t5-small\"\n",
    "            self.planner = T5MealPlanner(model_name)\n",
    "            self.tokenizer = self.planner.tokenizer\n",
    "        else:\n",
    "            raise ValueError(f\"Currently only T5 is fully implemented. Got: {model_type}\")\n",
    "\n",
    "        # Determine device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.planner.model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "\n",
    "    def build_knowledge_graph(self, foods: List[Food], patients: List[Patient]):\n",
    "        \"\"\"Build the knowledge graph from data\"\"\"\n",
    "        print(f\"Building knowledge graph with {len(foods)} foods and {len(patients)} patients...\")\n",
    "        for food in foods:\n",
    "            self.knowledge_graph.add_food_node(food)\n",
    "        for patient in patients:\n",
    "            self.knowledge_graph.add_patient_node(patient)\n",
    "        graph_data = self.knowledge_graph.to_pytorch_geometric()\n",
    "        return graph_data.to(self.device) # Move graph data to device\n",
    "\n",
    "\n",
    "    def get_food_recommendations(self, patient: Patient) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get food recommendations based on patient profile and knowledge graph\"\"\"\n",
    "        recommendations = {\n",
    "            'breakfast': [],\n",
    "            'lunch': [],\n",
    "            'dinner': [],\n",
    "            'snacks': []\n",
    "        }\n",
    "\n",
    "        # Use knowledge graph to get suitable foods\n",
    "        breakfast_categories = ['grains', 'fruits', 'dairy', 'nuts']\n",
    "        lunch_dinner_categories = ['grains', 'vegetables', 'legumes', 'dairy']\n",
    "        snack_categories = ['fruits', 'nuts', 'dairy']\n",
    "\n",
    "        # Get foods from each category\n",
    "        for cat in breakfast_categories:\n",
    "            if cat in self.knowledge_graph.food_names_by_category:\n",
    "                foods = self.knowledge_graph.food_names_by_category[cat]\n",
    "                recommendations['breakfast'].extend(foods[:2])  # Take first 2 from each category\n",
    "\n",
    "        for cat in lunch_dinner_categories:\n",
    "            if cat in self.knowledge_graph.food_names_by_category:\n",
    "                foods = self.knowledge_graph.food_names_by_category[cat]\n",
    "                recommendations['lunch'].extend(foods[:2])\n",
    "                recommendations['dinner'].extend(foods[2:4] if len(foods) > 2 else foods[:2])\n",
    "\n",
    "        for cat in snack_categories:\n",
    "            if cat in self.knowledge_graph.food_names_by_category:\n",
    "                foods = self.knowledge_graph.food_names_by_category[cat]\n",
    "                recommendations['snacks'].extend(foods[:1])\n",
    "\n",
    "        # Limit recommendations\n",
    "        recommendations['breakfast'] = recommendations['breakfast'][:4]\n",
    "        recommendations['lunch'] = recommendations['lunch'][:5]\n",
    "        recommendations['dinner'] = recommendations['dinner'][:5]\n",
    "        recommendations['snacks'] = recommendations['snacks'][:2]\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def fine_tune(self, train_dataset: AyurvedaMealPlanDataset,\n",
    "                  val_dataset: AyurvedaMealPlanDataset = None,\n",
    "                  output_dir: str = \"./ayurveda_meal_planner\",\n",
    "                  num_epochs: int = 3,\n",
    "                  batch_size: int = 4,\n",
    "                  learning_rate: float = 5e-5):\n",
    "        \"\"\"Fine-tune the pretrained model\"\"\"\n",
    "\n",
    "        print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=50,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            eval_steps=100 if val_dataset else None,\n",
    "            eval_strategy=\"steps\" if val_dataset else \"no\", # Changed evaluation_strategy to eval_strategy\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\" if val_dataset else None,\n",
    "            greater_is_better=False,\n",
    "            fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "            dataloader_pin_memory=False,\n",
    "            report_to=\"none\",  # Disable wandb/tensorboard\n",
    "            prediction_loss_only=False, # Ensure predictions are returned for metric calculation\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.planner.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics if val_dataset else None, # Add compute_metrics\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"✓ Model fine-tuned and saved to {output_dir}\")\n",
    "\n",
    "    def generate_meal_plan(self, patient: Patient, day: int,\n",
    "                          graph_data: Data = None,\n",
    "                          max_length: int = 256,\n",
    "                          temperature: float = 0.9,\n",
    "                          use_knowledge_graph: bool = True) -> str:\n",
    "        \"\"\"Generate meal plan for a patient\"\"\"\n",
    "\n",
    "        # Ensure graph_data is on the correct device\n",
    "        if graph_data is not None and graph_data.x.device != self.device:\n",
    "            graph_data = graph_data.to(self.device)\n",
    "\n",
    "        if use_knowledge_graph and not self.knowledge_graph.food_names_by_category:\n",
    "            # If no foods in graph, create some sample recommendations\n",
    "            print(\"⚠ No foods in knowledge graph, using default recommendations\")\n",
    "            return self._generate_default_plan(patient, day)\n",
    "\n",
    "        # Format input\n",
    "        input_text = self.planner.format_patient_input(patient, day)\n",
    "\n",
    "        # Tokenize and move to device\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.planner.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                min_length=20,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # If generation fails, use knowledge graph recommendations\n",
    "        if not self._has_valid_content(generated_text) and use_knowledge_graph:\n",
    "            recommendations = self.get_food_recommendations(patient)\n",
    "            generated_text = self._format_recommendations(recommendations)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    def _generate_default_plan(self, patient: Patient, day: int) -> str:\n",
    "        \"\"\"Generate a default meal plan based on patient profile\"\"\"\n",
    "        # Basic meal suggestions based on prakriti\n",
    "        vata_foods = {\n",
    "            'breakfast': ['oatmeal', 'warm milk', 'almonds', 'dates'],\n",
    "            'lunch': ['rice', 'moong dal', 'ghee', 'cooked vegetables'],\n",
    "            'dinner': ['khichdi', 'soup', 'bread', 'cooked spinach'],\n",
    "            'snacks': ['banana', 'soaked almonds']\n",
    "        }\n",
    "\n",
    "        pitta_foods = {\n",
    "            'breakfast': ['coconut water', 'sweet fruits', 'milk', 'cereal'],\n",
    "            'lunch': ['basmati rice', 'green vegetables', 'cucumber', 'yogurt'],\n",
    "            'dinner': ['quinoa', 'salad', 'sweet potato', 'green beans'],\n",
    "            'snacks': ['watermelon', 'coconut']\n",
    "        }\n",
    "\n",
    "        kapha_foods = {\n",
    "            'breakfast': ['honey water', 'light breakfast', 'berries', 'green tea'],\n",
    "            'lunch': ['millet', 'bitter vegetables', 'spices', 'legumes'],\n",
    "            'dinner': ['barley soup', 'steamed vegetables', 'ginger tea'],\n",
    "            'snacks': ['apple', 'pear']\n",
    "        }\n",
    "\n",
    "        # Select based on prakriti\n",
    "        prakriti = patient.prakriti.lower()\n",
    "        if 'vata' in prakriti:\n",
    "            foods = vata_foods\n",
    "        elif 'pitta' in prakriti:\n",
    "            foods = pitta_foods\n",
    "        elif 'kapha' in prakriti:\n",
    "            foods = kapha_foods\n",
    "        else:\n",
    "            # Mix of all\n",
    "            foods = {\n",
    "                'breakfast': vata_foods['breakfast'][:2] + pitta_foods['breakfast'][:2],\n",
    "                'lunch': vata_foods['lunch'][:2] + pitta_foods['lunch'][:2],\n",
    "                'dinner': kapha_foods['dinner'][:2] + vata_foods['dinner'][:2],\n",
    "                'snacks': pitta_foods['snacks']\n",
    "            }\n",
    "\n",
    "        return self._format_recommendations(foods)\n",
    "\n",
    "    def _format_recommendations(self, recommendations: Dict[str, List[str]]) -> str:\n",
    "        \"\"\"Format recommendations into text\"\"\"\n",
    "        text = \"\"\n",
    "        if recommendations.get('breakfast'):\n",
    "            text += f\"breakfast: {', '.join(recommendations['breakfast'])} \"\n",
    "        if recommendations.get('lunch'):\n",
    "            text += f\"lunch: {', '.join(recommendations['lunch'])} \"\n",
    "        if recommendations.get('dinner'):\n",
    "            text += f\"dinner: {', '.join(recommendations['dinner'])} \"\n",
    "        if recommendations.get('snacks'):\n",
    "            text += f\"snacks: {', '.join(recommendations['snacks'])}\"\n",
    "        return text.strip()\n",
    "\n",
    "    def _has_valid_content(self, text: str) -> bool:\n",
    "        \"\"\"Check if generated text has valid meal content\"\"\"\n",
    "        return any(meal in text.lower() for meal in ['breakfast', 'lunch', 'dinner', 'snacks'])\n",
    "\n",
    "    def parse_generated_plan(self, generated_text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Parse the generated text into structured meal plan\"\"\"\n",
    "        plan = {\n",
    "            'breakfast': [],\n",
    "            'lunch': [],\n",
    "            'dinner': [],\n",
    "            'snacks': []\n",
    "        }\n",
    "\n",
    "        text = generated_text.lower()\n",
    "\n",
    "        # Try different parsing strategies\n",
    "\n",
    "        # Strategy 1: Look for meal keywords followed by colon\n",
    "        for meal_type in ['breakfast', 'lunch', 'dinner', 'snacks']:\n",
    "            if f'{meal_type}:' in text:\n",
    "                # Find the section after the meal type\n",
    "                start = text.index(f'{meal_type}:') + len(f'{meal_type}:')\n",
    "                # Find the next meal type or end of string\n",
    "                end = len(text)\n",
    "                for next_meal in ['breakfast', 'lunch', 'dinner', 'snacks']:\n",
    "                    if next_meal != meal_type and f'{next_meal}:' in text[start:]:\n",
    "                        next_pos = text.index(f'{next_meal}:', start)\n",
    "                        if next_pos < end:\n",
    "                            end = next_pos\n",
    "\n",
    "                # Extract and clean the items\n",
    "                section = text[start:end].strip()\n",
    "                # Remove any tags\n",
    "                section = section.replace('</', ' ').replace('<', ' ')\n",
    "                # Split by comma or common separators\n",
    "                items = [item.strip() for item in section.split(',')]\n",
    "                # Clean items\n",
    "                cleaned_items = []\n",
    "                for item in items:\n",
    "                    # Remove extra spaces and special characters\n",
    "                    item = ' '.join(item.split())\n",
    "                    # Remove trailing periods or special chars\n",
    "                    item = item.rstrip('.,;')\n",
    "                    if item and len(item) > 1 and not item.startswith('/'):\n",
    "                        cleaned_items.append(item)\n",
    "\n",
    "                plan[meal_type] = cleaned_items[:5]  # Limit to 5 items per meal\n",
    "\n",
    "        # Strategy 2: If no meals found with colon, look for tags\n",
    "        if not any(plan.values()):\n",
    "            if '<breakfast>' in text and '</breakfast>' in text:\n",
    "                section = text.split('<breakfast>')[1].split('</breakfast>')[0]\n",
    "                plan['breakfast'] = [item.strip() for item in section.split(',') if item.strip()][:5]\n",
    "\n",
    "            if '<lunch>' in text and '</lunch>' in text:\n",
    "                section = text.split('<lunch>')[1].split('</lunch>')[0]\n",
    "                plan['lunch'] = [item.strip() for item in section.split(',') if item.strip()][:5]\n",
    "\n",
    "            if '<dinner>' in text and '</dinner>' in text:\n",
    "                section = text.split('<dinner>')[1].split('</dinner>')[0]\n",
    "                plan['dinner'] = [item.strip() for item in section.split(',') if item.strip()][:5]\n",
    "\n",
    "            if '<snacks>' in text and '</snacks>' in text:\n",
    "                section = text.split('<snacks>')[1].split('</snacks>')[0]\n",
    "                plan['snacks'] = [item.strip() for item in section.split(',') if item.strip()][:5]\n",
    "\n",
    "        # If still no meals found, use default plan\n",
    "        if not any(plan.values()):\n",
    "            plan = {\n",
    "                'breakfast': ['oatmeal', 'fruits', 'milk'],\n",
    "                'lunch': ['rice', 'dal', 'vegetables', 'yogurt'],\n",
    "                'dinner': ['chapati', 'vegetables', 'soup'],\n",
    "                'snacks': ['nuts', 'fruits']\n",
    "            }\n",
    "\n",
    "        return plan\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Main function to run the Ayurveda meal planning system\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    HF_TOKEN = os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HF_TOKEN')\n",
    "    FOODS_CSV = \"/content/drive/MyDrive/AyruAhaar-datasets/foods.csv\"\n",
    "    PATIENTS_CSV = \"/content/drive/MyDrive/AyruAhaar-datasets/patients.csv\"\n",
    "    PLANS_CSV = \"/content/drive/MyDrive/AyruAhaar-datasets/doctor_plans.csv\"\n",
    "\n",
    "    MODEL_TYPE = \"t5\"  # Currently only T5 is fully implemented\n",
    "    MODEL_NAME = \"t5-small\"  # Using smaller model for faster inference\n",
    "    DO_TRAIN = True  # Set to True to fine-tune\n",
    "    OUTPUT_DIR = \"./ayurveda_meal_planner\"\n",
    "    EPOCHS = 2  # Reduced for faster training\n",
    "    BATCH_SIZE = 1  # Smaller batch size\n",
    "    LEARNING_RATE = 3e-4  # Higher learning rate for small dataset\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌿 Ayurveda Meal Planning System\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Login to Hugging Face if token available\n",
    "    if HF_TOKEN:\n",
    "        huggingface_login(HF_TOKEN)\n",
    "\n",
    "    # Check if files exist\n",
    "    print(\"\\n📁 Checking data files...\")\n",
    "    for file_path, file_name in [(FOODS_CSV, \"foods\"), (PATIENTS_CSV, \"patients\"), (PLANS_CSV, \"doctor_plans\")]:\n",
    "        if Path(file_path).exists():\n",
    "            print(f\"  ✓ {file_name} CSV found\")\n",
    "        else:\n",
    "            print(f\"  ✗ {file_name} CSV not found at {file_path}\")\n",
    "            print(\"\\n⚠ Creating sample data for demonstration...\")\n",
    "            # Create sample data if files don't exist\n",
    "            create_sample_data(FOODS_CSV, PATIENTS_CSV, PLANS_CSV)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\n📊 Loading data...\")\n",
    "    try:\n",
    "        foods = load_foods_csv(FOODS_CSV)\n",
    "        patients = load_patients_csv(PATIENTS_CSV)\n",
    "        plans = load_doctor_plans_csv(PLANS_CSV)\n",
    "        print(f\"  ✓ Loaded {len(foods)} foods\")\n",
    "        print(f\"  ✓ Loaded {len(patients)} patients\")\n",
    "        print(f\"  ✓ Loaded {len(plans)} meal plans\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error loading data: {e}\")\n",
    "        print(\"\\n⚠ Creating sample data for demonstration...\")\n",
    "        create_sample_data(FOODS_CSV, PATIENTS_CSV, PLANS_CSV)\n",
    "        foods = load_foods_csv(FOODS_CSV)\n",
    "        patients = load_patients_csv(PATIENTS_CSV)\n",
    "        plans = load_doctor_plans_csv(PLANS_CSV)\n",
    "\n",
    "    # Initialize engine\n",
    "    print(\"\\n🤖 Initializing AI engine...\")\n",
    "    engine = HybridNeuralEngine(model_type=MODEL_TYPE, model_name=MODEL_NAME)\n",
    "    print(f\"  ✓ Model loaded: {MODEL_NAME}\")\n",
    "\n",
    "    # Build knowledge graph\n",
    "    print(\"\\n🔗 Building knowledge graph...\")\n",
    "    graph_data = engine.build_knowledge_graph(foods, patients)\n",
    "    print(f\"  ✓ Graph built with {graph_data.x.shape[0]} nodes\")\n",
    "\n",
    "    # Prepare dataset\n",
    "    if DO_TRAIN and len(plans) > 0:\n",
    "        print(\"\\n📚 Preparing training dataset...\")\n",
    "        dataset = AyurvedaMealPlanDataset(patients, plans, engine.tokenizer, model_type=MODEL_TYPE)\n",
    "\n",
    "        # Split into train and validation\n",
    "        train_size = int(0.9 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "\n",
    "        if train_size > 0:\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size]\n",
    "            )\n",
    "\n",
    "            print(f\"  ✓ Train set: {len(train_dataset)} samples\")\n",
    "            print(f\"  ✓ Validation set: {len(val_dataset)} samples\")\n",
    "\n",
    "            # Fine-tune the model\n",
    "            print(\"\\n🎯 Fine-tuning model...\")\n",
    "            engine.fine_tune(\n",
    "                train_dataset,\n",
    "                val_dataset if val_size > 0 else None,\n",
    "                output_dir=OUTPUT_DIR,\n",
    "                num_epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "            )\n",
    "\n",
    "    # Generate meal plans\n",
    "    print(\"\\n🍽️ Generating meal plans...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Generate for first 5 patients or all if less than 5\n",
    "    num_patients = min(5, len(patients))\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        patient = patients[i]\n",
    "        print(f\"\\n👤 Patient {patient.id}:\")\n",
    "        print(f\"   Age: {patient.age}, Gender: {patient.gender}\")\n",
    "        print(f\"   BMI: {patient.bmi:.1f}, Prakriti: {patient.prakriti}\")\n",
    "        if patient.health_conditions:\n",
    "            print(f\"   Conditions: {', '.join(patient.health_conditions)}\")\n",
    "\n",
    "        # Generate plan\n",
    "        generated_text = engine.generate_meal_plan(patient, day=1, graph_data=graph_data)\n",
    "        parsed_plan = engine.parse_generated_plan(generated_text)\n",
    "\n",
    "        print(f\"\\n   📅 Day 1 Meal Plan:\")\n",
    "        print(f\"   🌅 Breakfast: {', '.join(parsed_plan['breakfast']) if parsed_plan['breakfast'] else 'None'}\")\n",
    "        print(f\"   ☀️  Lunch: {', '.join(parsed_plan['lunch']) if parsed_plan['lunch'] else 'None'}\")\n",
    "        print(f\"   🌙 Dinner: {', '.join(parsed_plan['dinner']) if parsed_plan['dinner'] else 'None'}\")\n",
    "        print(f\"   🍎 Snacks: {', '.join(parsed_plan['snacks']) if parsed_plan['snacks'] else 'None'}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ Meal plan generation complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def create_sample_data(foods_path: str, patients_path: str, plans_path: str):\n",
    "    \"\"\"Create sample CSV files if they don't exist\"\"\"\n",
    "\n",
    "    # Create directories if needed\n",
    "    for path in [foods_path, patients_path, plans_path]:\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Sample foods data\n",
    "    foods_data = {\n",
    "        'id': ['F001', 'F002', 'F003', 'F004', 'F005'],\n",
    "        'name': ['Rice', 'Moong Dal', 'Ghee', 'Turmeric Milk', 'Almonds'],\n",
    "        'category': ['grains', 'legumes', 'oils', 'dairy', 'nuts'],\n",
    "        'calories': [130, 347, 900, 80, 579],\n",
    "        'protein': [2.7, 24, 0, 3.5, 21],\n",
    "        'carbs': [28, 63, 0, 12, 22],\n",
    "        'fats': [0.3, 1.2, 100, 3, 50],\n",
    "        'fiber': [0.4, 16, 0, 0, 12],\n",
    "        'rasa': ['sweet', 'sweet', 'sweet', 'sweet', 'sweet'],\n",
    "        'virya': ['cooling', 'cooling', 'cooling', 'heating', 'heating'],\n",
    "        'vipaka': ['sweet', 'sweet', 'sweet', 'sweet', 'sweet'],\n",
    "        'dosha_effects': ['vata:-,pitta:-,kapha:+', 'vata:-,pitta:-,kapha:-',\n",
    "                         'vata:-,pitta:-,kapha:+', 'vata:-,pitta:-,kapha:-',\n",
    "                         'vata:-,pitta:+,kapha:+'],\n",
    "        'health_tags': ['digestion', 'protein|digestion', 'immunity', 'sleep|immunity', 'brain|heart'],\n",
    "        'guna': ['light|soft', 'light|dry', 'heavy|oily', 'light|oily', 'heavy|oily']\n",
    "    }\n",
    "\n",
    "    # Sample patients data\n",
    "    patients_data = {\n",
    "        'id': ['P001', 'P002', 'P003'],\n",
    "        'age': [35, 28, 45],\n",
    "        'gender': ['male', 'female', 'male'],\n",
    "        'weight': [70, 60, 80],\n",
    "        'height': [175, 165, 180],\n",
    "        'bmi': [22.9, 22.0, 24.7],\n",
    "        'lifestyle': ['moderate', 'active', 'sedentary'],\n",
    "        'prakriti': ['vata', 'pitta', 'kapha'],\n",
    "        'health_conditions': ['', 'acidity', 'diabetes|obesity'],\n",
    "        'allergies': ['', 'nuts', ''],\n",
    "        'preferred_cuisine': ['indian', 'indian', 'indian']\n",
    "    }\n",
    "\n",
    "    # Sample meal plans data\n",
    "    plans_data = {\n",
    "        'patient_id': ['P001', 'P001', 'P002', 'P002', 'P003', 'P003'],\n",
    "        'day': [1, 2, 1, 2, 1, 2],\n",
    "        'breakfast': ['Rice porridge|Almonds|Milk', 'Oatmeal|Dates|Ghee',\n",
    "                     'Fruit salad|Yogurt', 'Cereal|Coconut water',\n",
    "                     'Green tea|Apple', 'Herbal tea|Berries'],\n",
    "        'lunch': ['Rice|Moong dal|Ghee|Vegetables', 'Khichdi|Yogurt|Salad',\n",
    "                 'Quinoa|Green vegetables|Cucumber', 'Rice|Dal|Steamed vegetables',\n",
    "                 'Millet|Bitter gourd|Spiced buttermilk', 'Barley|Mixed vegetables|Green salad'],\n",
    "        'dinner': ['Soup|Bread|Cooked vegetables', 'Rice|Dal|Spinach',\n",
    "                  'Sweet potato|Green beans|Salad', 'Pasta|Vegetables|Soup',\n",
    "                  'Vegetable soup|Brown rice', 'Clear soup|Steamed vegetables'],\n",
    "        'snacks': ['Banana|Soaked almonds', 'Dates|Warm milk',\n",
    "\n",
    "\n",
    "                  'Watermelon|Coconut water', 'Apple|Herbal tea',\n",
    "                  'Pear|Green tea', 'Cucumber|Carrot sticks'],\n",
    "        'restrictions': ['', '', 'no nuts', 'no nuts', 'low sugar', 'low sugar'],\n",
    "        'doctor_notes': ['Increase warm foods', 'Focus on grounding foods',\n",
    "                        'Cooling foods recommended', 'Avoid heating foods',\n",
    "                        'Light foods, avoid heavy meals', 'Increase metabolism']\n",
    "    }\n",
    "\n",
    "    # Save CSVs\n",
    "    pd.DataFrame(foods_data).to_csv(foods_path, index=False)\n",
    "    pd.DataFrame(patients_data).to_csv(patients_path, index=False)\n",
    "    pd.DataFrame(plans_data).to_csv(plans_path, index=False)\n",
    "\n",
    "    print(f\"  ✓ Created sample data files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
